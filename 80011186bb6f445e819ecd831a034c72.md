---
jupyter:
  kaggle:
    accelerator: nvidiaTeslaT4
    dataSources:
    - databundleVersionId: 7895811
      sourceId: 71608
      sourceType: competition
    dockerImageVersionId: 30665
    isGpuEnabled: true
    isInternetEnabled: true
    language: python
    sourceType: notebook
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.11.5
  nbformat: 4
  nbformat_minor: 5
---

::: {.cell .code execution_count="3" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:06.618385Z\",\"iopub.status.busy\":\"2024-03-15T22:47:06.617619Z\",\"iopub.status.idle\":\"2024-03-15T22:47:06.622672Z\",\"shell.execute_reply\":\"2024-03-15T22:47:06.621500Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:06.618354Z\"}" trusted="true"}
``` python
EPOCHS = 50
BATCH_SIZE = 32
```
:::

::: {.cell .code execution_count="4" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:08.405424Z\",\"iopub.status.busy\":\"2024-03-15T22:47:08.404762Z\",\"iopub.status.idle\":\"2024-03-15T22:47:08.409295Z\",\"shell.execute_reply\":\"2024-03-15T22:47:08.408370Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:08.405394Z\"}" trusted="true"}
``` python
import numpy as np
import pandas as pd
import os
```
:::

::: {.cell .code execution_count="5" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:09.671881Z\",\"iopub.status.busy\":\"2024-03-15T22:47:09.671087Z\",\"iopub.status.idle\":\"2024-03-15T22:47:22.503845Z\",\"shell.execute_reply\":\"2024-03-15T22:47:22.502837Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:09.671851Z\"}" trusted="true"}
``` python
import tensorflow as tf
```

::: {.output .stream .stderr}
    2024-03-15 22:47:11.781310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
    2024-03-15 22:47:11.781444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
    2024-03-15 22:47:11.953177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
:::
:::

::: {.cell .code execution_count="6" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:42.717630Z\",\"iopub.status.busy\":\"2024-03-15T22:47:42.716923Z\",\"iopub.status.idle\":\"2024-03-15T22:47:42.750431Z\",\"shell.execute_reply\":\"2024-03-15T22:47:42.749455Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:42.717593Z\"}" trusted="true"}
``` python
#reading ground truth

df = pd.read_csv('/kaggle/input/aiml-general-championship/KCDH2024_Training_GroundTruth.csv')
print(df.head())
print(len(df))
print(df.columns)
```

::: {.output .stream .stdout}
              image  MEL  NV  BCC  AKIEC  BKL  DF  VASC
    0  ISIC_0024306    0   1    0      0    0   0     0
    1  ISIC_0024307    0   1    0      0    0   0     0
    2  ISIC_0024308    0   1    0      0    0   0     0
    3  ISIC_0024309    0   1    0      0    0   0     0
    4  ISIC_0024310    1   0    0      0    0   0     0
    10015
    Index(['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'], dtype='object')
:::
:::

::: {.cell .code execution_count="7" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:45.191080Z\",\"iopub.status.busy\":\"2024-03-15T22:47:45.190380Z\",\"iopub.status.idle\":\"2024-03-15T22:47:45.196224Z\",\"shell.execute_reply\":\"2024-03-15T22:47:45.195207Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:45.191039Z\"}" trusted="true"}
``` python
label_names = ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']
label_names = sorted(label_names) #sorting
print(label_names)
```

::: {.output .stream .stdout}
    ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC']
:::
:::

::: {.cell .code execution_count="8" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:47.696876Z\",\"iopub.status.busy\":\"2024-03-15T22:47:47.696074Z\",\"iopub.status.idle\":\"2024-03-15T22:47:47.712909Z\",\"shell.execute_reply\":\"2024-03-15T22:47:47.711832Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:47.696826Z\"}" trusted="true"}
``` python
#adding a jpg extension to file names in csv file
df['image'] = df['image'].apply(lambda x: x+ '.jpg')
print(df.head())
```

::: {.output .stream .stdout}
                  image  MEL  NV  BCC  AKIEC  BKL  DF  VASC
    0  ISIC_0024306.jpg    0   1    0      0    0   0     0
    1  ISIC_0024307.jpg    0   1    0      0    0   0     0
    2  ISIC_0024308.jpg    0   1    0      0    0   0     0
    3  ISIC_0024309.jpg    0   1    0      0    0   0     0
    4  ISIC_0024310.jpg    1   0    0      0    0   0     0
:::
:::

::: {.cell .code execution_count="9" execution="{\"iopub.execute_input\":\"2024-03-15T22:47:49.336218Z\",\"iopub.status.busy\":\"2024-03-15T22:47:49.335533Z\",\"iopub.status.idle\":\"2024-03-15T22:47:49.343028Z\",\"shell.execute_reply\":\"2024-03-15T22:47:49.342015Z\",\"shell.execute_reply.started\":\"2024-03-15T22:47:49.336187Z\"}" trusted="true"}
``` python
df = df[:9600]
len(df)
```

::: {.output .execute_result execution_count="9"}
    9600
:::
:::

::: {.cell .code execution_count="10" execution="{\"iopub.execute_input\":\"2024-03-15T22:48:19.024688Z\",\"iopub.status.busy\":\"2024-03-15T22:48:19.023713Z\",\"iopub.status.idle\":\"2024-03-15T22:48:19.031476Z\",\"shell.execute_reply\":\"2024-03-15T22:48:19.030468Z\",\"shell.execute_reply.started\":\"2024-03-15T22:48:19.024646Z\"}" trusted="true"}
``` python
import shutil

data_dir = '/kaggle/working/lesions'

# Empty directory to prevent FileExistsError if the function is run several times
if os.path.exists(data_dir):
  shutil.rmtree(data_dir)

# Create the empty dir for each skin lesion
for label in label_names:
    os.makedirs(os.path.join(data_dir, label)) # e.g. /kaggle/working/lesions/MEL
```
:::

::: {.cell .code execution_count="11" execution="{\"iopub.execute_input\":\"2024-03-15T22:49:05.077296Z\",\"iopub.status.busy\":\"2024-03-15T22:49:05.076567Z\",\"iopub.status.idle\":\"2024-03-15T22:50:40.334837Z\",\"shell.execute_reply\":\"2024-03-15T22:50:40.333888Z\",\"shell.execute_reply.started\":\"2024-03-15T22:49:05.077264Z\"}" trusted="true"}
``` python
from shutil import copyfile
from tqdm import tqdm

# Iterate over dataframe and move images to correct folders
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f'Copying HAM10000 dataset images..'):
    # Get the image pathname
    image_name = row['image']  # e.g. ISIC_0024306.jpg
    
    # Find which labels are hot for this image (i.e. equal to 1)
    # This returns a list of indices, should just be one
    hot_label = row[row == 1].index.tolist()[0]  # e.g. MEL
    
#     print(image_name, hot_labels)  # ISIC_0024306.jpg NV

    # Copy the image to the right label directory
    src_path = os.path.join('/kaggle/input/aiml-general-championship/KCDH2024_Training_Input_10K/KCDH2024_Training_Input_10K', image_name)
    dst_path = os.path.join(data_dir, hot_label, image_name)
    
#     print(src_path)
#     print(dst_path)
    
    copyfile(src_path, dst_path)

#     if index > 10:
#         break
```

::: {.output .stream .stderr}
    Copying HAM10000 dataset images..: 100%|██████████| 9600/9600 [01:35<00:00, 100.80it/s]
:::
:::

::: {.cell .code execution_count="47" execution="{\"iopub.execute_input\":\"2024-03-15T23:27:41.947810Z\",\"iopub.status.busy\":\"2024-03-15T23:27:41.947137Z\",\"iopub.status.idle\":\"2024-03-15T23:27:41.959797Z\",\"shell.execute_reply\":\"2024-03-15T23:27:41.958774Z\",\"shell.execute_reply.started\":\"2024-03-15T23:27:41.947777Z\"}" trusted="true"}
``` python
tot = 0
for label in label_names:
    cnt_label = len(os.listdir(os.path.join(data_dir, label)))
    print(f"There are {cnt_label} images with label {label}.")
    tot += cnt_label
print(f"\nThere are {tot} total images across all labels.")
```

::: {.output .stream .stdout}
    There are 327 images with label AKIEC.
    There are 495 images with label BCC.
    There are 1055 images with label BKL.
    There are 113 images with label DF.
    There are 1015 images with label MEL.
    There are 6457 images with label NV.
    There are 138 images with label VASC.

    There are 9600 total images across all labels.
:::
:::

::: {.cell .markdown}
## Calculating Class weights
:::

::: {.cell .code execution_count="48" execution="{\"iopub.execute_input\":\"2024-03-15T23:27:45.642443Z\",\"iopub.status.busy\":\"2024-03-15T23:27:45.642045Z\",\"iopub.status.idle\":\"2024-03-15T23:27:45.661770Z\",\"shell.execute_reply\":\"2024-03-15T23:27:45.660551Z\",\"shell.execute_reply.started\":\"2024-03-15T23:27:45.642414Z\"}" trusted="true"}
``` python
# Start weights at zero
num_classes = len(label_names)
weights = [0] * num_classes  # e.g. [0, 0, 0, .. 0]

tot = 0
for idx, label in enumerate(label_names):
    cnt_label = len(os.listdir(os.path.join(data_dir, label)))
    weights[idx] = cnt_label  # really a count right now
    tot += cnt_label

# Calculate class frequencies (we'll use this later to initialize our bias terms)
class_frequencies = weights
class_frequencies = [ w / tot for w in weights ]  # [0.018897364771151177, 0.0297041 ...

# Now we have the total counts across all classes, and the 
# weights array is actually counts at the moment.
# Less frequent classes have higher weights, inversely proportional
weights = [1.0 / cnt if cnt != 0 else 0 for cnt in weights]


# And we can scale these weights by total_cnt / num_classes
# which will keep the loss to a similar magnitude

# (and doesn't affect the weight proportions)

# To think about this math another way, imagine we just had two classes
# and they were already evenly proportioned (but still using class_weights
# for sake of example).  Say total_cnt = 100 and neg_class_cnt=pos_class_cnt=50 
# weight0 = total_cnt / neg_class_cnt = 2
# weight1 = total_cnt / pos_class_cnt = 2
# But we can normalize these by dividing again by 2 to give 1:1 weights.
# So same idea but for more than 2 classes.
if tot != 0:
    weights = [tot * w / num_classes for w in weights]
else:
    # Handle the case where tot is zero (e.g., by setting weights to a default value)
    weights = [1.0 / num_classes] * num_classes


# Now assign to a class_weight dictionary that Keras expects
class_weight = {}
for i in range(num_classes):
    class_weight[i] = weights[i]
    print(f"Weight for class {i}: " + '{:.2f}'.format(weights[i]))
```

::: {.output .stream .stdout}
    Weight for class 0: 4.19
    Weight for class 1: 2.77
    Weight for class 2: 1.30
    Weight for class 3: 12.14
    Weight for class 4: 1.35
    Weight for class 5: 0.21
    Weight for class 6: 9.94
:::
:::

::: {.cell .code execution_count="49" execution="{\"iopub.execute_input\":\"2024-03-15T23:27:47.039969Z\",\"iopub.status.busy\":\"2024-03-15T23:27:47.039626Z\",\"iopub.status.idle\":\"2024-03-15T23:27:47.044492Z\",\"shell.execute_reply\":\"2024-03-15T23:27:47.043508Z\",\"shell.execute_reply.started\":\"2024-03-15T23:27:47.039941Z\"}" trusted="true"}
``` python
# The image resizing is determined by what model of EfficientNet we want to use
# Let's use the largest EfficientNet that doesn't need to scale up the images.
# But because of the new Benign image set, we're limited to EfficientNetB0.
# Still good for a demo, but in real life you'd want to use probably the best
# model you could since don't care about inference speed for medical applications,
# but just best accuracy!
IMG_SIZE = 224  # EfficientNetB0, 5.3M parameters
```
:::

::: {.cell .code execution_count="50" execution="{\"iopub.execute_input\":\"2024-03-15T23:27:49.185540Z\",\"iopub.status.busy\":\"2024-03-15T23:27:49.184693Z\",\"iopub.status.idle\":\"2024-03-15T23:27:49.831624Z\",\"shell.execute_reply\":\"2024-03-15T23:27:49.830622Z\",\"shell.execute_reply.started\":\"2024-03-15T23:27:49.185504Z\"}" trusted="true"}
``` python
# Now that images are in correct disk folder structure, we can create 
# a tf data dataset easily.  Can return the Train and val/test datasets all at once.
train_ds, val_and_test_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    labels="inferred",
    label_mode="categorical",   # will use categorical_crossentropy loss later
    class_names=label_names,
    batch_size=BATCH_SIZE,
    image_size=(IMG_SIZE, IMG_SIZE),
    shuffle=True,
    seed=7,
    validation_split=0.1,       # Set aside 10% for val & test
    subset="both",              # Returns a tuple
    # Benign are square but Malignant are rectangle, we'll do a center
    # crop that makes a IMG_SIZE x IMG_SIZE square for all images
    crop_to_aspect_ratio=True,
)
```

::: {.output .stream .stdout}
    Found 9600 files belonging to 7 classes.
    Using 8640 files for training.
    Using 960 files for validation.
:::
:::

::: {.cell .code execution_count="51" execution="{\"iopub.execute_input\":\"2024-03-15T23:27:50.197071Z\",\"iopub.status.busy\":\"2024-03-15T23:27:50.196717Z\",\"iopub.status.idle\":\"2024-03-15T23:27:52.064016Z\",\"shell.execute_reply\":\"2024-03-15T23:27:52.063113Z\",\"shell.execute_reply.started\":\"2024-03-15T23:27:50.197032Z\"}" trusted="true"}
``` python
val_and_test_ds_size = len(list(val_and_test_ds))

# Calculate the number of samples for each split
val_size = int(val_and_test_ds_size * 0.5)
test_size = val_and_test_ds_size - val_size

# Split the dataset
val_ds = val_and_test_ds.take(val_size)
test_ds = val_and_test_ds.skip(val_size)
```
:::

::: {.cell .markdown}
## Visualizing data
:::

::: {.cell .code execution_count="52" execution="{\"iopub.execute_input\":\"2024-03-15T23:28:24.368012Z\",\"iopub.status.busy\":\"2024-03-15T23:28:24.367647Z\",\"iopub.status.idle\":\"2024-03-15T23:28:27.407863Z\",\"shell.execute_reply\":\"2024-03-15T23:28:27.406953Z\",\"shell.execute_reply.started\":\"2024-03-15T23:28:24.367981Z\"}" trusted="true"}
``` python
import matplotlib.pyplot as plt

n_rows = 3
n_cols = 5

plt.figure(figsize=(n_cols * 2.0, n_rows * 2.0))

# Get a batch to work with
for images_batch, labels_batch in train_ds.take(1):
    pass

for row in range(n_rows):
    for col in range(n_cols):
        index = n_cols * row + col
        plt.subplot(n_rows, n_cols, index + 1)
        
        # Grab one image and convert it to NumPy type values
        img = images_batch[index, :, :, :].numpy()
        plt.imshow(img) #, cmap="binary", interpolation="nearest")
        
        # For the labels, we need to turn a length-8 one-hot EagerTensor
        # into the string label
        label_row = labels_batch[index, :]
       
        # Find the class label number with argmax and convert to NumPy type
        class_label = tf.argmax(label_row).numpy()
        
        # Look up the list
        class_label = label_names[class_label]
        
        # Use it on the plot
        plt.title(class_label)
plt.subplots_adjust(wspace=0.2, hspace=0.5)
plt.show()       
```

::: {.output .display_data}
![](vertopal_80011186bb6f445e819ecd831a034c72/cc61446e9b460d69cf6c3f9274dd006376966439.png)
:::
:::

::: {.cell .code execution_count="53" execution="{\"iopub.execute_input\":\"2024-03-15T23:28:32.330379Z\",\"iopub.status.busy\":\"2024-03-15T23:28:32.329480Z\",\"iopub.status.idle\":\"2024-03-15T23:28:32.342917Z\",\"shell.execute_reply\":\"2024-03-15T23:28:32.341868Z\",\"shell.execute_reply.started\":\"2024-03-15T23:28:32.330341Z\"}" trusted="true"}
``` python
# A mole is gonna look valid in any rotation or flipping, since there isn't a 
# left-to-right "direction" to how cells grow.  Thus we can do lots of augmentation.
image_augmentation = tf.keras.models.Sequential([
    tf.keras.layers.RandomRotation(factor=0.15),
    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
    tf.keras.layers.RandomFlip(mode="horizontal_and_vertical", seed=7)
    ])
```
:::

::: {.cell .code execution_count="55" execution="{\"iopub.execute_input\":\"2024-03-15T23:28:43.523416Z\",\"iopub.status.busy\":\"2024-03-15T23:28:43.522660Z\",\"iopub.status.idle\":\"2024-03-15T23:28:43.539410Z\",\"shell.execute_reply\":\"2024-03-15T23:28:43.538627Z\",\"shell.execute_reply.started\":\"2024-03-15T23:28:43.523385Z\"}" trusted="true"}
``` python
# Cache dataset for performance and prefetch so CPU can do work while GPU 
# training on current batch.  Train is already shuffled earlier.
train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
```
:::

::: {.cell .code execution_count="56" execution="{\"iopub.execute_input\":\"2024-03-15T23:28:45.472852Z\",\"iopub.status.busy\":\"2024-03-15T23:28:45.471904Z\",\"iopub.status.idle\":\"2024-03-15T23:28:45.476745Z\",\"shell.execute_reply\":\"2024-03-15T23:28:45.475772Z\",\"shell.execute_reply.started\":\"2024-03-15T23:28:45.472820Z\"}" trusted="true"}
``` python
from tensorflow.keras.applications import EfficientNetB0
```
:::

::: {.cell .code execution_count="57" execution="{\"iopub.execute_input\":\"2024-03-15T23:28:47.856117Z\",\"iopub.status.busy\":\"2024-03-15T23:28:47.855482Z\",\"iopub.status.idle\":\"2024-03-15T23:28:47.877525Z\",\"shell.execute_reply\":\"2024-03-15T23:28:47.876708Z\",\"shell.execute_reply.started\":\"2024-03-15T23:28:47.856084Z\"}" trusted="true"}
``` python
# Going to start by 
# 1. retraining the classifier (head) 
# 2. then we'll fine-tune all weights next

METRICS = [
    tf.keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),
    tf.keras.metrics.TruePositives(name='tp'),
    tf.keras.metrics.TrueNegatives(name='tn'),
    tf.keras.metrics.FalsePositives(name='fp'),
    tf.keras.metrics.FalseNegatives(name='fn'),
    tf.keras.metrics.Precision(name='precision'),
    tf.keras.metrics.Recall(name='recall'),
    tf.keras.metrics.F1Score(name='f1'),
    tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),  # not trustworthy since imbalanced!
]

def build_model(num_classes, metrics=METRICS, output_bias=None):
    
    # Can set initial output biases to something other than zero for faster convergence
    if output_bias is not None:
        output_bias = tf.keras.initializers.Constant(output_bias)
    
    # Inputs and augmentation
    inputs = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    aug_inputs = image_augmentation(inputs)
    
    # Base model
    model = EfficientNetB0(include_top=False, input_tensor=aug_inputs, weights="imagenet")

    # Freeze the pretrained weights
    model.trainable = False

    # Rebuild top like in original paper
    x = tf.keras.layers.GlobalAveragePooling2D(name="avg_pool")(model.output)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.2, name="top_dropout")(x)
    outputs = tf.keras.layers.Dense(num_classes, 
                                    activation="softmax",
                                    # bias otherwise defaults to zeros
                                    bias_initializer=output_bias,
                                    name="pred")(x)

    # Compile Model
    model = tf.keras.Model(inputs, outputs, name="EfficientNet")
    
    # Using a large learning rate for this part since most weights are fixed
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)
    
    model.compile(
        optimizer=optimizer, loss="categorical_crossentropy", metrics=metrics,
    )
    
    return model
```
:::

::: {.cell .code execution_count="60" execution="{\"iopub.execute_input\":\"2024-03-15T23:29:22.910595Z\",\"iopub.status.busy\":\"2024-03-15T23:29:22.910308Z\",\"iopub.status.idle\":\"2024-03-15T23:29:23.988496Z\",\"shell.execute_reply\":\"2024-03-15T23:29:23.987690Z\",\"shell.execute_reply.started\":\"2024-03-15T23:29:22.910569Z\"}" trusted="true"}
``` python
model = build_model(num_classes)
# model.summary()
```
:::

::: {.cell .code execution_count="61" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:04.337558Z\",\"iopub.status.busy\":\"2024-03-15T23:30:04.337177Z\",\"iopub.status.idle\":\"2024-03-15T23:30:07.758802Z\",\"shell.execute_reply\":\"2024-03-15T23:30:07.757866Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:04.337528Z\"}" trusted="true"}
``` python
# Get a batch to work with
for image_batch, label_batch in train_ds.take(1):
    pass

results = model.evaluate(image_batch, label_batch, batch_size=BATCH_SIZE, verbose=0)
print("Loss: {:0.4f}".format(results[0]))
```

::: {.output .stream .stdout}
    Loss: 2.0163
:::
:::

::: {.cell .code execution_count="62" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:12.079733Z\",\"iopub.status.busy\":\"2024-03-15T23:30:12.079363Z\",\"iopub.status.idle\":\"2024-03-15T23:30:12.085818Z\",\"shell.execute_reply\":\"2024-03-15T23:30:12.084743Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:12.079704Z\"}" trusted="true"}
``` python
# We calculated the class frequencies earlier when doing the class_weights
class_frequencies = np.array(class_frequencies)

# Making priors that match the data distribution for faster convergence
class_log_odds = np.log(class_frequencies / (1 - class_frequencies))

print(class_log_odds)
```

::: {.output .stream .stdout}
    [-3.34490206 -2.91202123 -2.09180555 -4.4302899  -2.13512788  0.71998706
     -4.22778537]
:::
:::

::: {.cell .code execution_count="63" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:13.653804Z\",\"iopub.status.busy\":\"2024-03-15T23:30:13.653439Z\",\"iopub.status.idle\":\"2024-03-15T23:30:14.718928Z\",\"shell.execute_reply\":\"2024-03-15T23:30:14.717941Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:13.653779Z\"}" trusted="true"}
``` python
model = build_model(num_classes, output_bias=class_log_odds)
# model.summary()
```
:::

::: {.cell .code execution_count="64" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:16.802779Z\",\"iopub.status.busy\":\"2024-03-15T23:30:16.802353Z\",\"iopub.status.idle\":\"2024-03-15T23:30:19.711517Z\",\"shell.execute_reply\":\"2024-03-15T23:30:19.710563Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:16.802746Z\"}" trusted="true"}
``` python
# Now see the loss on that same batch with better initialized bias terms
results = model.evaluate(image_batch, label_batch, batch_size=BATCH_SIZE, verbose=0)
print("Loss: {:0.4f}".format(results[0]))
```

::: {.output .stream .stdout}
    Loss: 1.6828
:::
:::

::: {.cell .code execution_count="67" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:44.100055Z\",\"iopub.status.busy\":\"2024-03-15T23:30:44.099694Z\",\"iopub.status.idle\":\"2024-03-15T23:30:44.105536Z\",\"shell.execute_reply\":\"2024-03-15T23:30:44.104590Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:44.100025Z\"}" trusted="true"}
``` python
# Use early stopping to train as long as possible
early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True,
)

# And Reduce LR on Plateau to lower learning rate if stagnating
reduce_lr_on_plateau_cb = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,         # reduce by half (divide by 2)
    patience=3,         # must be lower than patience of early stopping
    verbose=1,          # tell me when rate changed
)
EPOCHS = 50
```
:::

::: {.cell .code execution_count="68" execution="{\"iopub.execute_input\":\"2024-03-15T23:30:46.519006Z\",\"iopub.status.busy\":\"2024-03-15T23:30:46.518407Z\",\"iopub.status.idle\":\"2024-03-15T23:43:59.581040Z\",\"shell.execute_reply\":\"2024-03-15T23:43:59.580031Z\",\"shell.execute_reply.started\":\"2024-03-15T23:30:46.518975Z\"}" trusted="true"}
``` python
history = model.fit(train_ds, 
                    epochs=EPOCHS, 
                    validation_data=val_ds,
                    class_weight=class_weight,  # use our class weights here
                    callbacks=[early_stopping_cb, reduce_lr_on_plateau_cb],
                   )
```

::: {.output .stream .stdout}
    Epoch 1/50
:::

::: {.output .stream .stderr}
    2024-03-15 23:30:52.334195: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/EfficientNet_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
:::

::: {.output .stream .stdout}
    270/270 ━━━━━━━━━━━━━━━━━━━━ 34s 99ms/step - categorical_accuracy: 0.4058 - categorical_crossentropy: 3.1995 - f1: 0.2506 - fn: 2645.6863 - fp: 2315.5425 - loss: 3.4796 - precision: 0.4169 - recall: 0.3802 - tn: 23795.7500 - tp: 1706.1956 - val_categorical_accuracy: 0.6771 - val_categorical_crossentropy: 1.2018 - val_f1: 0.3224 - val_fn: 170.0000 - val_fp: 128.0000 - val_loss: 1.2018 - val_precision: 0.7078 - val_recall: 0.6458 - val_tn: 2752.0000 - val_tp: 310.0000 - learning_rate: 0.0100
    Epoch 2/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.4876 - categorical_crossentropy: 2.6730 - f1: 0.3361 - fn: 2328.5720 - fp: 2093.9077 - loss: 2.6421 - precision: 0.4978 - recall: 0.4731 - tn: 24017.3828 - tp: 2023.3099 - val_categorical_accuracy: 0.5500 - val_categorical_crossentropy: 2.0622 - val_f1: 0.3512 - val_fn: 226.0000 - val_fp: 188.0000 - val_loss: 2.0622 - val_precision: 0.5747 - val_recall: 0.5292 - val_tn: 2692.0000 - val_tp: 254.0000 - learning_rate: 0.0100
    Epoch 3/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.5197 - categorical_crossentropy: 2.1529 - f1: 0.3591 - fn: 2164.4060 - fp: 1846.2472 - loss: 2.0928 - precision: 0.5411 - recall: 0.5008 - tn: 24265.0449 - tp: 2187.4761 - val_categorical_accuracy: 0.5208 - val_categorical_crossentropy: 2.2382 - val_f1: 0.3629 - val_fn: 242.0000 - val_fp: 203.0000 - val_loss: 2.2382 - val_precision: 0.5397 - val_recall: 0.4958 - val_tn: 2677.0000 - val_tp: 238.0000 - learning_rate: 0.0100
    Epoch 4/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - categorical_accuracy: 0.5315 - categorical_crossentropy: 1.8811 - f1: 0.3695 - fn: 2096.4111 - fp: 1720.8000 - loss: 1.8336 - precision: 0.5588 - recall: 0.5097 - tn: 24295.1992 - tp: 2239.5889
    Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 41s 79ms/step - categorical_accuracy: 0.5315 - categorical_crossentropy: 1.8811 - f1: 0.3695 - fn: 2104.2324 - fp: 1727.1439 - loss: 1.8335 - precision: 0.5588 - recall: 0.5097 - tn: 24384.1484 - tp: 2247.6494 - val_categorical_accuracy: 0.5083 - val_categorical_crossentropy: 2.0081 - val_f1: 0.3548 - val_fn: 249.0000 - val_fp: 199.0000 - val_loss: 2.0081 - val_precision: 0.5372 - val_recall: 0.4812 - val_tn: 2681.0000 - val_tp: 231.0000 - learning_rate: 0.0100
    Epoch 5/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 80ms/step - categorical_accuracy: 0.5590 - categorical_crossentropy: 1.6270 - f1: 0.3897 - fn: 2028.2067 - fp: 1572.0591 - loss: 1.5016 - precision: 0.5849 - recall: 0.5248 - tn: 24539.2324 - tp: 2323.6753 - val_categorical_accuracy: 0.6313 - val_categorical_crossentropy: 1.2929 - val_f1: 0.4682 - val_fn: 188.0000 - val_fp: 135.0000 - val_loss: 1.2929 - val_precision: 0.6838 - val_recall: 0.6083 - val_tn: 2745.0000 - val_tp: 292.0000 - learning_rate: 0.0050
    Epoch 6/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.5786 - categorical_crossentropy: 1.4030 - f1: 0.4144 - fn: 2033.9963 - fp: 1476.3322 - loss: 1.2822 - precision: 0.6182 - recall: 0.5397 - tn: 24634.9590 - tp: 2317.8855 - val_categorical_accuracy: 0.6146 - val_categorical_crossentropy: 1.3438 - val_f1: 0.4755 - val_fn: 206.0000 - val_fp: 142.0000 - val_loss: 1.3438 - val_precision: 0.6587 - val_recall: 0.5708 - val_tn: 2738.0000 - val_tp: 274.0000 - learning_rate: 0.0050
    Epoch 7/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6138 - categorical_crossentropy: 1.2302 - f1: 0.4522 - fn: 1900.5277 - fp: 1296.7933 - loss: 1.1680 - precision: 0.6563 - recall: 0.5638 - tn: 24814.4980 - tp: 2451.3542 - val_categorical_accuracy: 0.6500 - val_categorical_crossentropy: 1.1556 - val_f1: 0.4681 - val_fn: 188.0000 - val_fp: 132.0000 - val_loss: 1.1556 - val_precision: 0.6887 - val_recall: 0.6083 - val_tn: 2748.0000 - val_tp: 292.0000 - learning_rate: 0.0050
    Epoch 8/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.5979 - categorical_crossentropy: 1.2030 - f1: 0.4373 - fn: 1947.6458 - fp: 1324.8782 - loss: 1.1299 - precision: 0.6488 - recall: 0.5545 - tn: 24786.4141 - tp: 2404.2361 - val_categorical_accuracy: 0.6083 - val_categorical_crossentropy: 1.1598 - val_f1: 0.4257 - val_fn: 213.0000 - val_fp: 142.0000 - val_loss: 1.1598 - val_precision: 0.6528 - val_recall: 0.5562 - val_tn: 2738.0000 - val_tp: 267.0000 - learning_rate: 0.0050
    Epoch 9/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.5810 - categorical_crossentropy: 1.2465 - f1: 0.4264 - fn: 2070.6716 - fp: 1355.6014 - loss: 1.1194 - precision: 0.6294 - recall: 0.5236 - tn: 24755.6895 - tp: 2281.2104 - val_categorical_accuracy: 0.6646 - val_categorical_crossentropy: 1.0022 - val_f1: 0.4680 - val_fn: 175.0000 - val_fp: 118.0000 - val_loss: 1.0022 - val_precision: 0.7210 - val_recall: 0.6354 - val_tn: 2762.0000 - val_tp: 305.0000 - learning_rate: 0.0050
    Epoch 10/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.5845 - categorical_crossentropy: 1.2643 - f1: 0.4257 - fn: 2026.3506 - fp: 1335.3875 - loss: 1.0808 - precision: 0.6316 - recall: 0.5312 - tn: 24775.9043 - tp: 2325.5312 - val_categorical_accuracy: 0.6375 - val_categorical_crossentropy: 1.1327 - val_f1: 0.4814 - val_fn: 188.0000 - val_fp: 126.0000 - val_loss: 1.1327 - val_precision: 0.6986 - val_recall: 0.6083 - val_tn: 2754.0000 - val_tp: 292.0000 - learning_rate: 0.0050
    Epoch 11/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.5963 - categorical_crossentropy: 1.1868 - f1: 0.4342 - fn: 1983.0701 - fp: 1293.0332 - loss: 1.1041 - precision: 0.6486 - recall: 0.5486 - tn: 24818.2578 - tp: 2368.8118 - val_categorical_accuracy: 0.6271 - val_categorical_crossentropy: 1.1591 - val_f1: 0.4498 - val_fn: 197.0000 - val_fp: 134.0000 - val_loss: 1.1591 - val_precision: 0.6787 - val_recall: 0.5896 - val_tn: 2746.0000 - val_tp: 283.0000 - learning_rate: 0.0050
    Epoch 12/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - categorical_accuracy: 0.6029 - categorical_crossentropy: 1.1551 - f1: 0.4461 - fn: 1960.4778 - fp: 1287.3223 - loss: 1.0907 - precision: 0.6474 - recall: 0.5489 - tn: 24728.6777 - tp: 2375.5222
    Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6028 - categorical_crossentropy: 1.1552 - f1: 0.4460 - fn: 1967.9336 - fp: 1292.0627 - loss: 1.0907 - precision: 0.6474 - recall: 0.5489 - tn: 24819.2285 - tp: 2383.9482 - val_categorical_accuracy: 0.6417 - val_categorical_crossentropy: 1.0848 - val_f1: 0.4305 - val_fn: 186.0000 - val_fp: 134.0000 - val_loss: 1.0848 - val_precision: 0.6869 - val_recall: 0.6125 - val_tn: 2746.0000 - val_tp: 294.0000 - learning_rate: 0.0050
    Epoch 13/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.6138 - categorical_crossentropy: 1.1454 - f1: 0.4509 - fn: 1941.0516 - fp: 1202.4613 - loss: 1.0308 - precision: 0.6672 - recall: 0.5546 - tn: 24908.8301 - tp: 2410.8303 - val_categorical_accuracy: 0.6042 - val_categorical_crossentropy: 1.1696 - val_f1: 0.4503 - val_fn: 208.0000 - val_fp: 140.0000 - val_loss: 1.1696 - val_precision: 0.6602 - val_recall: 0.5667 - val_tn: 2740.0000 - val_tp: 272.0000 - learning_rate: 0.0025
    Epoch 14/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6362 - categorical_crossentropy: 1.0135 - f1: 0.4824 - fn: 1848.0073 - fp: 1080.9741 - loss: 0.9169 - precision: 0.7034 - recall: 0.5781 - tn: 25030.3164 - tp: 2503.8745 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 1.0224 - val_f1: 0.4801 - val_fn: 202.0000 - val_fp: 122.0000 - val_loss: 1.0224 - val_precision: 0.6950 - val_recall: 0.5792 - val_tn: 2758.0000 - val_tp: 278.0000 - learning_rate: 0.0025
    Epoch 15/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.6281 - categorical_crossentropy: 1.0817 - f1: 0.4708 - fn: 1905.4502 - fp: 1080.0443 - loss: 0.9975 - precision: 0.6958 - recall: 0.5608 - tn: 25031.2480 - tp: 2446.4316 - val_categorical_accuracy: 0.6542 - val_categorical_crossentropy: 0.9853 - val_f1: 0.4765 - val_fn: 195.0000 - val_fp: 106.0000 - val_loss: 0.9853 - val_precision: 0.7289 - val_recall: 0.5938 - val_tn: 2774.0000 - val_tp: 285.0000 - learning_rate: 0.0025
    Epoch 16/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.6192 - categorical_crossentropy: 1.0504 - f1: 0.4569 - fn: 1963.1587 - fp: 1109.2694 - loss: 0.9933 - precision: 0.6852 - recall: 0.5501 - tn: 25002.0215 - tp: 2388.7231 - val_categorical_accuracy: 0.6438 - val_categorical_crossentropy: 1.0279 - val_f1: 0.4673 - val_fn: 197.0000 - val_fp: 116.0000 - val_loss: 1.0279 - val_precision: 0.7093 - val_recall: 0.5896 - val_tn: 2764.0000 - val_tp: 283.0000 - learning_rate: 0.0025
    Epoch 17/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6413 - categorical_crossentropy: 1.0093 - f1: 0.4835 - fn: 1908.1365 - fp: 1078.7454 - loss: 0.9420 - precision: 0.6986 - recall: 0.5672 - tn: 25032.5469 - tp: 2443.7454 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 1.0596 - val_f1: 0.4864 - val_fn: 204.0000 - val_fp: 129.0000 - val_loss: 1.0596 - val_precision: 0.6815 - val_recall: 0.5750 - val_tn: 2751.0000 - val_tp: 276.0000 - learning_rate: 0.0025
    Epoch 18/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6437 - categorical_crossentropy: 1.0085 - f1: 0.4920 - fn: 1878.0406 - fp: 1071.4575 - loss: 0.9053 - precision: 0.7010 - recall: 0.5724 - tn: 25039.8340 - tp: 2473.8413 - val_categorical_accuracy: 0.6500 - val_categorical_crossentropy: 0.9843 - val_f1: 0.4539 - val_fn: 191.0000 - val_fp: 111.0000 - val_loss: 0.9843 - val_precision: 0.7225 - val_recall: 0.6021 - val_tn: 2769.0000 - val_tp: 289.0000 - learning_rate: 0.0025
    Epoch 19/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6274 - categorical_crossentropy: 1.0342 - f1: 0.4816 - fn: 1930.6458 - fp: 1083.4871 - loss: 0.9101 - precision: 0.6880 - recall: 0.5551 - tn: 25027.8047 - tp: 2421.2361 - val_categorical_accuracy: 0.6250 - val_categorical_crossentropy: 1.0414 - val_f1: 0.4811 - val_fn: 203.0000 - val_fp: 121.0000 - val_loss: 1.0414 - val_precision: 0.6960 - val_recall: 0.5771 - val_tn: 2759.0000 - val_tp: 277.0000 - learning_rate: 0.0025
    Epoch 20/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6416 - categorical_crossentropy: 1.0033 - f1: 0.4909 - fn: 1882.7712 - fp: 1070.5239 - loss: 0.9067 - precision: 0.6989 - recall: 0.5679 - tn: 25040.7676 - tp: 2469.1106 - val_categorical_accuracy: 0.6375 - val_categorical_crossentropy: 1.0270 - val_f1: 0.4948 - val_fn: 199.0000 - val_fp: 110.0000 - val_loss: 1.0270 - val_precision: 0.7187 - val_recall: 0.5854 - val_tn: 2770.0000 - val_tp: 281.0000 - learning_rate: 0.0025
    Epoch 21/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - categorical_accuracy: 0.6448 - categorical_crossentropy: 0.9915 - f1: 0.4858 - fn: 1864.1333 - fp: 1024.4963 - loss: 0.8655 - precision: 0.7080 - recall: 0.5725 - tn: 24991.5039 - tp: 2471.8667
    Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6448 - categorical_crossentropy: 0.9916 - f1: 0.4857 - fn: 1871.2767 - fp: 1028.4650 - loss: 0.8656 - precision: 0.7079 - recall: 0.5725 - tn: 25082.8262 - tp: 2480.6052 - val_categorical_accuracy: 0.6292 - val_categorical_crossentropy: 1.0199 - val_f1: 0.4622 - val_fn: 203.0000 - val_fp: 115.0000 - val_loss: 1.0199 - val_precision: 0.7066 - val_recall: 0.5771 - val_tn: 2765.0000 - val_tp: 277.0000 - learning_rate: 0.0025
    Epoch 22/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6333 - categorical_crossentropy: 1.0200 - f1: 0.4748 - fn: 1887.7933 - fp: 1062.3875 - loss: 0.9008 - precision: 0.6969 - recall: 0.5637 - tn: 25048.9043 - tp: 2464.0886 - val_categorical_accuracy: 0.6333 - val_categorical_crossentropy: 1.0416 - val_f1: 0.4824 - val_fn: 198.0000 - val_fp: 122.0000 - val_loss: 1.0416 - val_precision: 0.6980 - val_recall: 0.5875 - val_tn: 2758.0000 - val_tp: 282.0000 - learning_rate: 0.0012
    Epoch 23/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6543 - categorical_crossentropy: 0.9534 - f1: 0.5113 - fn: 1832.5018 - fp: 973.9778 - loss: 0.8534 - precision: 0.7223 - recall: 0.5773 - tn: 25137.3145 - tp: 2519.3801 - val_categorical_accuracy: 0.6271 - val_categorical_crossentropy: 1.0446 - val_f1: 0.4701 - val_fn: 203.0000 - val_fp: 123.0000 - val_loss: 1.0446 - val_precision: 0.6925 - val_recall: 0.5771 - val_tn: 2757.0000 - val_tp: 277.0000 - learning_rate: 0.0012
    Epoch 24/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.6551 - categorical_crossentropy: 0.9569 - f1: 0.5121 - fn: 1812.9852 - fp: 1006.3173 - loss: 0.8337 - precision: 0.7208 - recall: 0.5876 - tn: 25104.9746 - tp: 2538.8967 - val_categorical_accuracy: 0.6500 - val_categorical_crossentropy: 0.9802 - val_f1: 0.4842 - val_fn: 195.0000 - val_fp: 112.0000 - val_loss: 0.9802 - val_precision: 0.7179 - val_recall: 0.5938 - val_tn: 2768.0000 - val_tp: 285.0000 - learning_rate: 0.0012
    Epoch 25/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6389 - categorical_crossentropy: 0.9716 - f1: 0.4859 - fn: 1871.8488 - fp: 982.4650 - loss: 0.8532 - precision: 0.7155 - recall: 0.5682 - tn: 25128.8262 - tp: 2480.0332 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 1.0028 - val_f1: 0.4968 - val_fn: 198.0000 - val_fp: 120.0000 - val_loss: 1.0028 - val_precision: 0.7015 - val_recall: 0.5875 - val_tn: 2760.0000 - val_tp: 282.0000 - learning_rate: 0.0012
    Epoch 26/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 41s 79ms/step - categorical_accuracy: 0.6514 - categorical_crossentropy: 0.9710 - f1: 0.4942 - fn: 1825.5350 - fp: 1002.2067 - loss: 0.8773 - precision: 0.7224 - recall: 0.5829 - tn: 25109.0840 - tp: 2526.3469 - val_categorical_accuracy: 0.6271 - val_categorical_crossentropy: 1.0602 - val_f1: 0.5143 - val_fn: 202.0000 - val_fp: 119.0000 - val_loss: 1.0602 - val_precision: 0.7003 - val_recall: 0.5792 - val_tn: 2761.0000 - val_tp: 278.0000 - learning_rate: 0.0012
    Epoch 27/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - categorical_accuracy: 0.6596 - categorical_crossentropy: 0.9633 - f1: 0.5050 - fn: 1790.8481 - fp: 971.8370 - loss: 0.8538 - precision: 0.7219 - recall: 0.5877 - tn: 25044.1621 - tp: 2545.1519
    Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 22s 80ms/step - categorical_accuracy: 0.6596 - categorical_crossentropy: 0.9633 - f1: 0.5050 - fn: 1797.5498 - fp: 975.4982 - loss: 0.8538 - precision: 0.7219 - recall: 0.5877 - tn: 25135.7930 - tp: 2554.3320 - val_categorical_accuracy: 0.6604 - val_categorical_crossentropy: 1.0039 - val_f1: 0.4967 - val_fn: 195.0000 - val_fp: 115.0000 - val_loss: 1.0039 - val_precision: 0.7125 - val_recall: 0.5938 - val_tn: 2765.0000 - val_tp: 285.0000 - learning_rate: 0.0012
    Epoch 28/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6667 - categorical_crossentropy: 0.8920 - f1: 0.5197 - fn: 1753.6974 - fp: 902.8007 - loss: 0.7868 - precision: 0.7447 - recall: 0.5973 - tn: 25208.4902 - tp: 2598.1846 - val_categorical_accuracy: 0.6229 - val_categorical_crossentropy: 1.0677 - val_f1: 0.4776 - val_fn: 206.0000 - val_fp: 125.0000 - val_loss: 1.0677 - val_precision: 0.6867 - val_recall: 0.5708 - val_tn: 2755.0000 - val_tp: 274.0000 - learning_rate: 6.2500e-04
    Epoch 29/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6616 - categorical_crossentropy: 0.9464 - f1: 0.5173 - fn: 1816.0000 - fp: 915.3099 - loss: 0.8341 - precision: 0.7360 - recall: 0.5838 - tn: 25195.9824 - tp: 2535.8818 - val_categorical_accuracy: 0.6125 - val_categorical_crossentropy: 1.0770 - val_f1: 0.4843 - val_fn: 211.0000 - val_fp: 129.0000 - val_loss: 1.0770 - val_precision: 0.6759 - val_recall: 0.5604 - val_tn: 2751.0000 - val_tp: 269.0000 - learning_rate: 6.2500e-04
    Epoch 30/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - categorical_accuracy: 0.6548 - categorical_crossentropy: 0.9543 - f1: 0.5052 - fn: 1842.3926 - fp: 923.0296 - loss: 0.8480 - precision: 0.7309 - recall: 0.5757 - tn: 25092.9707 - tp: 2493.6074
    Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6548 - categorical_crossentropy: 0.9542 - f1: 0.5052 - fn: 1849.2251 - fp: 926.5129 - loss: 0.8480 - precision: 0.7309 - recall: 0.5756 - tn: 25184.7793 - tp: 2502.6567 - val_categorical_accuracy: 0.6479 - val_categorical_crossentropy: 0.9981 - val_f1: 0.5038 - val_fn: 195.0000 - val_fp: 109.0000 - val_loss: 0.9981 - val_precision: 0.7234 - val_recall: 0.5938 - val_tn: 2771.0000 - val_tp: 285.0000 - learning_rate: 6.2500e-04
    Epoch 31/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6589 - categorical_crossentropy: 0.9152 - f1: 0.5082 - fn: 1779.5941 - fp: 914.4834 - loss: 0.8220 - precision: 0.7348 - recall: 0.5880 - tn: 25196.8086 - tp: 2572.2878 - val_categorical_accuracy: 0.6521 - val_categorical_crossentropy: 1.0270 - val_f1: 0.5214 - val_fn: 198.0000 - val_fp: 117.0000 - val_loss: 1.0270 - val_precision: 0.7068 - val_recall: 0.5875 - val_tn: 2763.0000 - val_tp: 282.0000 - learning_rate: 3.1250e-04
    Epoch 32/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 41s 79ms/step - categorical_accuracy: 0.6673 - categorical_crossentropy: 0.9130 - f1: 0.5178 - fn: 1788.2435 - fp: 927.2067 - loss: 0.8120 - precision: 0.7389 - recall: 0.5900 - tn: 25184.0840 - tp: 2563.6384 - val_categorical_accuracy: 0.6354 - val_categorical_crossentropy: 1.0527 - val_f1: 0.4961 - val_fn: 199.0000 - val_fp: 120.0000 - val_loss: 1.0527 - val_precision: 0.7007 - val_recall: 0.5854 - val_tn: 2760.0000 - val_tp: 281.0000 - learning_rate: 3.1250e-04
    Epoch 33/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - categorical_accuracy: 0.6771 - categorical_crossentropy: 0.9028 - f1: 0.5408 - fn: 1726.2667 - fp: 902.9111 - loss: 0.7720 - precision: 0.7423 - recall: 0.6003 - tn: 25113.0898 - tp: 2609.7334
    Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 79ms/step - categorical_accuracy: 0.6771 - categorical_crossentropy: 0.9028 - f1: 0.5407 - fn: 1732.7306 - fp: 906.2657 - loss: 0.7721 - precision: 0.7422 - recall: 0.6003 - tn: 25205.0254 - tp: 2619.1514 - val_categorical_accuracy: 0.6458 - val_categorical_crossentropy: 1.0357 - val_f1: 0.5016 - val_fn: 195.0000 - val_fp: 123.0000 - val_loss: 1.0357 - val_precision: 0.6985 - val_recall: 0.5938 - val_tn: 2757.0000 - val_tp: 285.0000 - learning_rate: 3.1250e-04
    Epoch 34/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 21s 78ms/step - categorical_accuracy: 0.6591 - categorical_crossentropy: 0.9210 - f1: 0.5098 - fn: 1782.4281 - fp: 920.7491 - loss: 0.8238 - precision: 0.7384 - recall: 0.5898 - tn: 25190.5430 - tp: 2569.4539 - val_categorical_accuracy: 0.6271 - val_categorical_crossentropy: 1.0701 - val_f1: 0.4944 - val_fn: 202.0000 - val_fp: 129.0000 - val_loss: 1.0701 - val_precision: 0.6830 - val_recall: 0.5792 - val_tn: 2751.0000 - val_tp: 278.0000 - learning_rate: 1.5625e-04
:::
:::

::: {.cell .code execution_count="69" execution="{\"iopub.execute_input\":\"2024-03-15T23:45:39.433084Z\",\"iopub.status.busy\":\"2024-03-15T23:45:39.432086Z\",\"iopub.status.idle\":\"2024-03-15T23:45:39.441244Z\",\"shell.execute_reply\":\"2024-03-15T23:45:39.440105Z\",\"shell.execute_reply.started\":\"2024-03-15T23:45:39.433031Z\"}" trusted="true"}
``` python
import matplotlib.pyplot as plt
def plot_history(history):
    # Plot the loss and accuracy curves for training and validation 
    fig, ax = plt.subplots(2,1)
    ax[0].plot(history.history['loss'], color='b', label="Training loss")
    ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
    legend = ax[0].legend(loc='best', shadow=True)

    ax[1].plot(history.history['categorical_accuracy'], color='b', label="Training accuracy")
    ax[1].plot(history.history['val_categorical_accuracy'], color='r',label="Validation accuracy")
    legend = ax[1].legend(loc='best', shadow=True)
```
:::

::: {.cell .code execution_count="78" execution="{\"iopub.execute_input\":\"2024-03-15T23:52:20.636195Z\",\"iopub.status.busy\":\"2024-03-15T23:52:20.635318Z\",\"iopub.status.idle\":\"2024-03-15T23:52:21.083311Z\",\"shell.execute_reply\":\"2024-03-15T23:52:21.082388Z\",\"shell.execute_reply.started\":\"2024-03-15T23:52:20.636160Z\"}" trusted="true"}
``` python

plot_history(history)
```

::: {.output .display_data}
![](vertopal_80011186bb6f445e819ecd831a034c72/56cea0ec1ae0cff8d9d8be761ccc382b7bec6ff0.png)
:::
:::

::: {.cell .code execution_count="79" execution="{\"iopub.execute_input\":\"2024-03-15T23:52:49.642754Z\",\"iopub.status.busy\":\"2024-03-15T23:52:49.642387Z\",\"iopub.status.idle\":\"2024-03-15T23:52:49.658382Z\",\"shell.execute_reply\":\"2024-03-15T23:52:49.657502Z\",\"shell.execute_reply.started\":\"2024-03-15T23:52:49.642726Z\"}" trusted="true"}
``` python
def unfreeze_model(model, metrics=METRICS):
    
#     We unfreeze the top 20 layers while leaving BatchNorm layers frozen
    # Unfreeze all the layers, but leave BatchNorm layers frozen
#     for layer in model.layers[-20:]:
    for layer in model.layers:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = True

    # Use a much smaller learning rate now to start
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
    
    # Recompile to 'freeze' which layers are frozen (not trainable)
    model.compile(
        optimizer=optimizer, loss="categorical_crossentropy", metrics=metrics
    )

unfreeze_model(model)
```
:::

::: {.cell .code execution_count="80" execution="{\"iopub.execute_input\":\"2024-03-15T23:53:32.493698Z\",\"iopub.status.busy\":\"2024-03-15T23:53:32.493325Z\",\"iopub.status.idle\":\"2024-03-16T00:33:07.908861Z\",\"shell.execute_reply\":\"2024-03-16T00:33:07.907951Z\",\"shell.execute_reply.started\":\"2024-03-15T23:53:32.493670Z\"}" trusted="true"}
``` python
history = model.fit(train_ds, 
                    epochs=EPOCHS, 
                    validation_data=val_ds,
                    class_weight=class_weight,  # use our class weights here
                    callbacks=[early_stopping_cb, reduce_lr_on_plateau_cb],
                   )
```

::: {.output .stream .stdout}
    Epoch 1/50
:::

::: {.output .stream .stderr}
    2024-03-15 23:53:54.365877: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/EfficientNet_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
:::

::: {.output .stream .stdout}
    270/270 ━━━━━━━━━━━━━━━━━━━━ 88s 238ms/step - categorical_accuracy: 0.6529 - categorical_crossentropy: 0.9480 - f1: 0.5105 - fn: 1984.9225 - fp: 1091.4318 - loss: 0.8155 - precision: 0.7192 - recall: 0.5872 - tn: 27899.8594 - tp: 2846.9595 - val_categorical_accuracy: 0.6375 - val_categorical_crossentropy: 1.0072 - val_f1: 0.4863 - val_fn: 199.0000 - val_fp: 124.0000 - val_loss: 1.0072 - val_precision: 0.6938 - val_recall: 0.5854 - val_tn: 2756.0000 - val_tp: 281.0000 - learning_rate: 1.0000e-05
    Epoch 2/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6741 - categorical_crossentropy: 0.9038 - f1: 0.5346 - fn: 1695.1882 - fp: 963.8155 - loss: 0.7698 - precision: 0.7378 - recall: 0.6108 - tn: 25147.4766 - tp: 2656.6938 - val_categorical_accuracy: 0.6250 - val_categorical_crossentropy: 1.0177 - val_f1: 0.4829 - val_fn: 204.0000 - val_fp: 131.0000 - val_loss: 1.0177 - val_precision: 0.6781 - val_recall: 0.5750 - val_tn: 2749.0000 - val_tp: 276.0000 - learning_rate: 1.0000e-05
    Epoch 3/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6752 - categorical_crossentropy: 0.8978 - f1: 0.5380 - fn: 1678.4945 - fp: 936.4797 - loss: 0.7304 - precision: 0.7397 - recall: 0.6125 - tn: 25174.8125 - tp: 2673.3875 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 0.9966 - val_f1: 0.5183 - val_fn: 196.0000 - val_fp: 125.0000 - val_loss: 0.9966 - val_precision: 0.6944 - val_recall: 0.5917 - val_tn: 2755.0000 - val_tp: 284.0000 - learning_rate: 1.0000e-05
    Epoch 4/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6814 - categorical_crossentropy: 0.8695 - f1: 0.5541 - fn: 1632.7970 - fp: 967.6753 - loss: 0.6854 - precision: 0.7370 - recall: 0.6233 - tn: 25143.6172 - tp: 2719.0850 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 1.0304 - val_f1: 0.5728 - val_fn: 195.0000 - val_fp: 132.0000 - val_loss: 1.0304 - val_precision: 0.6835 - val_recall: 0.5938 - val_tn: 2748.0000 - val_tp: 285.0000 - learning_rate: 1.0000e-05
    Epoch 5/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6852 - categorical_crossentropy: 0.8639 - f1: 0.5522 - fn: 1580.3358 - fp: 919.4908 - loss: 0.6778 - precision: 0.7493 - recall: 0.6353 - tn: 25191.8008 - tp: 2771.5461 - val_categorical_accuracy: 0.6396 - val_categorical_crossentropy: 0.9964 - val_f1: 0.5450 - val_fn: 193.0000 - val_fp: 133.0000 - val_loss: 0.9964 - val_precision: 0.6833 - val_recall: 0.5979 - val_tn: 2747.0000 - val_tp: 287.0000 - learning_rate: 1.0000e-05
    Epoch 6/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6941 - categorical_crossentropy: 0.8480 - f1: 0.5756 - fn: 1550.0259 - fp: 927.1513 - loss: 0.6373 - precision: 0.7501 - recall: 0.6413 - tn: 25184.1406 - tp: 2801.8562 - val_categorical_accuracy: 0.6354 - val_categorical_crossentropy: 1.0101 - val_f1: 0.4977 - val_fn: 197.0000 - val_fp: 132.0000 - val_loss: 1.0101 - val_precision: 0.6819 - val_recall: 0.5896 - val_tn: 2748.0000 - val_tp: 283.0000 - learning_rate: 1.0000e-05
    Epoch 7/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.6940 - categorical_crossentropy: 0.8300 - f1: 0.5762 - fn: 1549.6494 - fp: 927.7048 - loss: 0.6409 - precision: 0.7522 - recall: 0.6433 - tn: 25183.5859 - tp: 2802.2324 - val_categorical_accuracy: 0.6667 - val_categorical_crossentropy: 0.9610 - val_f1: 0.5532 - val_fn: 181.0000 - val_fp: 129.0000 - val_loss: 0.9610 - val_precision: 0.6986 - val_recall: 0.6229 - val_tn: 2751.0000 - val_tp: 299.0000 - learning_rate: 1.0000e-05
    Epoch 8/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7059 - categorical_crossentropy: 0.8146 - f1: 0.5960 - fn: 1481.6790 - fp: 902.7712 - loss: 0.6337 - precision: 0.7577 - recall: 0.6577 - tn: 25208.5195 - tp: 2870.2029 - val_categorical_accuracy: 0.6583 - val_categorical_crossentropy: 0.9633 - val_f1: 0.5403 - val_fn: 190.0000 - val_fp: 125.0000 - val_loss: 0.9633 - val_precision: 0.6988 - val_recall: 0.6042 - val_tn: 2755.0000 - val_tp: 290.0000 - learning_rate: 1.0000e-05
    Epoch 9/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7125 - categorical_crossentropy: 0.7882 - f1: 0.5995 - fn: 1479.0037 - fp: 874.8192 - loss: 0.5836 - precision: 0.7668 - recall: 0.6588 - tn: 25236.4727 - tp: 2872.8782 - val_categorical_accuracy: 0.6292 - val_categorical_crossentropy: 1.0238 - val_f1: 0.5270 - val_fn: 195.0000 - val_fp: 140.0000 - val_loss: 1.0238 - val_precision: 0.6706 - val_recall: 0.5938 - val_tn: 2740.0000 - val_tp: 285.0000 - learning_rate: 1.0000e-05
    Epoch 10/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - categorical_accuracy: 0.7131 - categorical_crossentropy: 0.7773 - f1: 0.5953 - fn: 1471.8112 - fp: 878.8297 - loss: 0.5798 - precision: 0.7636 - recall: 0.6577 - tn: 25137.1699 - tp: 2864.1890
    Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7131 - categorical_crossentropy: 0.7773 - f1: 0.5953 - fn: 1477.1587 - fp: 882.0369 - loss: 0.5798 - precision: 0.7636 - recall: 0.6577 - tn: 25229.2539 - tp: 2874.7231 - val_categorical_accuracy: 0.6479 - val_categorical_crossentropy: 0.9712 - val_f1: 0.5384 - val_fn: 185.0000 - val_fp: 125.0000 - val_loss: 0.9712 - val_precision: 0.7024 - val_recall: 0.6146 - val_tn: 2755.0000 - val_tp: 295.0000 - learning_rate: 1.0000e-05
    Epoch 11/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7179 - categorical_crossentropy: 0.7688 - f1: 0.6067 - fn: 1450.8782 - fp: 898.1919 - loss: 0.5723 - precision: 0.7647 - recall: 0.6672 - tn: 25213.0996 - tp: 2901.0037 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9149 - val_f1: 0.5957 - val_fn: 183.0000 - val_fp: 122.0000 - val_loss: 0.9149 - val_precision: 0.7088 - val_recall: 0.6187 - val_tn: 2758.0000 - val_tp: 297.0000 - learning_rate: 5.0000e-06
    Epoch 12/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7235 - categorical_crossentropy: 0.7693 - f1: 0.6141 - fn: 1416.4095 - fp: 867.2029 - loss: 0.5657 - precision: 0.7728 - recall: 0.6741 - tn: 25244.0879 - tp: 2935.4724 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9416 - val_f1: 0.5893 - val_fn: 182.0000 - val_fp: 119.0000 - val_loss: 0.9416 - val_precision: 0.7146 - val_recall: 0.6208 - val_tn: 2761.0000 - val_tp: 298.0000 - learning_rate: 5.0000e-06
    Epoch 13/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7335 - categorical_crossentropy: 0.7441 - f1: 0.6316 - fn: 1428.1144 - fp: 865.9299 - loss: 0.5384 - precision: 0.7733 - recall: 0.6703 - tn: 25245.3613 - tp: 2923.7676 - val_categorical_accuracy: 0.6562 - val_categorical_crossentropy: 0.9613 - val_f1: 0.5677 - val_fn: 188.0000 - val_fp: 130.0000 - val_loss: 0.9613 - val_precision: 0.6919 - val_recall: 0.6083 - val_tn: 2750.0000 - val_tp: 292.0000 - learning_rate: 5.0000e-06
    Epoch 14/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - categorical_accuracy: 0.7301 - categorical_crossentropy: 0.7414 - f1: 0.6245 - fn: 1375.9186 - fp: 861.1148 - loss: 0.5344 - precision: 0.7746 - recall: 0.6824 - tn: 25154.8848 - tp: 2960.0815
    Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7301 - categorical_crossentropy: 0.7414 - f1: 0.6245 - fn: 1380.9115 - fp: 864.1882 - loss: 0.5344 - precision: 0.7747 - recall: 0.6824 - tn: 25247.1035 - tp: 2970.9705 - val_categorical_accuracy: 0.6625 - val_categorical_crossentropy: 0.9289 - val_f1: 0.5924 - val_fn: 179.0000 - val_fp: 123.0000 - val_loss: 0.9289 - val_precision: 0.7099 - val_recall: 0.6271 - val_tn: 2757.0000 - val_tp: 301.0000 - learning_rate: 5.0000e-06
    Epoch 15/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7328 - categorical_crossentropy: 0.7319 - f1: 0.6351 - fn: 1388.1365 - fp: 837.4133 - loss: 0.5145 - precision: 0.7807 - recall: 0.6816 - tn: 25273.8789 - tp: 2963.7454 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9209 - val_f1: 0.6069 - val_fn: 178.0000 - val_fp: 119.0000 - val_loss: 0.9209 - val_precision: 0.7173 - val_recall: 0.6292 - val_tn: 2761.0000 - val_tp: 302.0000 - learning_rate: 2.5000e-06
    Epoch 16/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7326 - categorical_crossentropy: 0.7177 - f1: 0.6407 - fn: 1352.4318 - fp: 825.3690 - loss: 0.5015 - precision: 0.7827 - recall: 0.6886 - tn: 25285.9219 - tp: 2999.4502 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9282 - val_f1: 0.5976 - val_fn: 179.0000 - val_fp: 126.0000 - val_loss: 0.9282 - val_precision: 0.7049 - val_recall: 0.6271 - val_tn: 2754.0000 - val_tp: 301.0000 - learning_rate: 2.5000e-06
    Epoch 17/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7443 - categorical_crossentropy: 0.6956 - f1: 0.6475 - fn: 1328.6864 - fp: 799.7380 - loss: 0.4836 - precision: 0.7954 - recall: 0.6983 - tn: 25311.5527 - tp: 3023.1956 - val_categorical_accuracy: 0.6687 - val_categorical_crossentropy: 0.9029 - val_f1: 0.5873 - val_fn: 179.0000 - val_fp: 119.0000 - val_loss: 0.9029 - val_precision: 0.7167 - val_recall: 0.6271 - val_tn: 2761.0000 - val_tp: 301.0000 - learning_rate: 2.5000e-06
    Epoch 18/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7450 - categorical_crossentropy: 0.7074 - f1: 0.6611 - fn: 1317.9852 - fp: 832.8339 - loss: 0.4940 - precision: 0.7853 - recall: 0.6961 - tn: 25278.4570 - tp: 3033.8967 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9107 - val_f1: 0.5778 - val_fn: 175.0000 - val_fp: 120.0000 - val_loss: 0.9107 - val_precision: 0.7176 - val_recall: 0.6354 - val_tn: 2760.0000 - val_tp: 305.0000 - learning_rate: 2.5000e-06
    Epoch 19/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7399 - categorical_crossentropy: 0.7044 - f1: 0.6529 - fn: 1321.7565 - fp: 811.9114 - loss: 0.4844 - precision: 0.7902 - recall: 0.6966 - tn: 25299.3809 - tp: 3030.1255 - val_categorical_accuracy: 0.6729 - val_categorical_crossentropy: 0.9035 - val_f1: 0.5666 - val_fn: 178.0000 - val_fp: 121.0000 - val_loss: 0.9035 - val_precision: 0.7139 - val_recall: 0.6292 - val_tn: 2759.0000 - val_tp: 302.0000 - learning_rate: 2.5000e-06
    Epoch 20/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7464 - categorical_crossentropy: 0.7038 - f1: 0.6593 - fn: 1320.6014 - fp: 798.5461 - loss: 0.4787 - precision: 0.7913 - recall: 0.6999 - tn: 25312.7461 - tp: 3031.2805 - val_categorical_accuracy: 0.6604 - val_categorical_crossentropy: 0.8977 - val_f1: 0.5686 - val_fn: 178.0000 - val_fp: 123.0000 - val_loss: 0.8977 - val_precision: 0.7106 - val_recall: 0.6292 - val_tn: 2757.0000 - val_tp: 302.0000 - learning_rate: 2.5000e-06
    Epoch 21/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 82s 229ms/step - categorical_accuracy: 0.7451 - categorical_crossentropy: 0.6982 - f1: 0.6524 - fn: 1323.8340 - fp: 832.1513 - loss: 0.4950 - precision: 0.7862 - recall: 0.6954 - tn: 25279.1406 - tp: 3028.0479 - val_categorical_accuracy: 0.6750 - val_categorical_crossentropy: 0.8906 - val_f1: 0.5842 - val_fn: 174.0000 - val_fp: 115.0000 - val_loss: 0.8906 - val_precision: 0.7268 - val_recall: 0.6375 - val_tn: 2765.0000 - val_tp: 306.0000 - learning_rate: 2.5000e-06
    Epoch 22/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7336 - categorical_crossentropy: 0.7082 - f1: 0.6399 - fn: 1333.4686 - fp: 817.6088 - loss: 0.5004 - precision: 0.7878 - recall: 0.6926 - tn: 25293.6836 - tp: 3018.4133 - val_categorical_accuracy: 0.6771 - val_categorical_crossentropy: 0.8966 - val_f1: 0.6080 - val_fn: 173.0000 - val_fp: 122.0000 - val_loss: 0.8966 - val_precision: 0.7156 - val_recall: 0.6396 - val_tn: 2758.0000 - val_tp: 307.0000 - learning_rate: 2.5000e-06
    Epoch 23/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7471 - categorical_crossentropy: 0.6872 - f1: 0.6579 - fn: 1296.7010 - fp: 793.3985 - loss: 0.4709 - precision: 0.7959 - recall: 0.7030 - tn: 25317.8926 - tp: 3055.1809 - val_categorical_accuracy: 0.6583 - val_categorical_crossentropy: 0.9425 - val_f1: 0.6086 - val_fn: 185.0000 - val_fp: 133.0000 - val_loss: 0.9425 - val_precision: 0.6893 - val_recall: 0.6146 - val_tn: 2747.0000 - val_tp: 295.0000 - learning_rate: 2.5000e-06
    Epoch 24/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7544 - categorical_crossentropy: 0.6787 - f1: 0.6725 - fn: 1256.8118 - fp: 773.4133 - loss: 0.4524 - precision: 0.7973 - recall: 0.7103 - tn: 25337.8789 - tp: 3095.0701 - val_categorical_accuracy: 0.6708 - val_categorical_crossentropy: 0.9053 - val_f1: 0.5763 - val_fn: 175.0000 - val_fp: 124.0000 - val_loss: 0.9053 - val_precision: 0.7110 - val_recall: 0.6354 - val_tn: 2756.0000 - val_tp: 305.0000 - learning_rate: 1.2500e-06
    Epoch 30/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7463 - categorical_crossentropy: 0.6681 - f1: 0.6675 - fn: 1286.5682 - fp: 800.8561 - loss: 0.4486 - precision: 0.7929 - recall: 0.7040 - tn: 25310.4355 - tp: 3065.3137 - val_categorical_accuracy: 0.6750 - val_categorical_crossentropy: 0.8958 - val_f1: 0.5752 - val_fn: 173.0000 - val_fp: 125.0000 - val_loss: 0.8958 - val_precision: 0.7106 - val_recall: 0.6396 - val_tn: 2755.0000 - val_tp: 307.0000 - learning_rate: 1.2500e-06
    Epoch 31/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - categorical_accuracy: 0.7529 - categorical_crossentropy: 0.6701 - f1: 0.6695 - fn: 1260.2667 - fp: 772.2000 - loss: 0.4530 - precision: 0.7998 - recall: 0.7086 - tn: 25243.8008 - tp: 3075.7334
    Epoch 31: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7529 - categorical_crossentropy: 0.6701 - f1: 0.6695 - fn: 1264.8413 - fp: 775.0406 - loss: 0.4530 - precision: 0.7998 - recall: 0.7086 - tn: 25336.2500 - tp: 3087.0405 - val_categorical_accuracy: 0.6854 - val_categorical_crossentropy: 0.8803 - val_f1: 0.5866 - val_fn: 174.0000 - val_fp: 123.0000 - val_loss: 0.8803 - val_precision: 0.7133 - val_recall: 0.6375 - val_tn: 2757.0000 - val_tp: 306.0000 - learning_rate: 1.2500e-06
    Epoch 32/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7471 - categorical_crossentropy: 0.6773 - f1: 0.6623 - fn: 1255.7897 - fp: 812.6937 - loss: 0.4508 - precision: 0.7909 - recall: 0.7101 - tn: 25298.5977 - tp: 3096.0923 - val_categorical_accuracy: 0.6771 - val_categorical_crossentropy: 0.8833 - val_f1: 0.5849 - val_fn: 172.0000 - val_fp: 122.0000 - val_loss: 0.8833 - val_precision: 0.7163 - val_recall: 0.6417 - val_tn: 2758.0000 - val_tp: 308.0000 - learning_rate: 6.2500e-07
    Epoch 33/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7524 - categorical_crossentropy: 0.6700 - f1: 0.6681 - fn: 1273.0148 - fp: 781.8229 - loss: 0.4779 - precision: 0.8015 - recall: 0.7073 - tn: 25329.4688 - tp: 3078.8672 - val_categorical_accuracy: 0.6812 - val_categorical_crossentropy: 0.8712 - val_f1: 0.5826 - val_fn: 174.0000 - val_fp: 120.0000 - val_loss: 0.8712 - val_precision: 0.7183 - val_recall: 0.6375 - val_tn: 2760.0000 - val_tp: 306.0000 - learning_rate: 6.2500e-07
    Epoch 34/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - categorical_accuracy: 0.7556 - categorical_crossentropy: 0.6664 - f1: 0.6762 - fn: 1259.9370 - fp: 774.1963 - loss: 0.4501 - precision: 0.7993 - recall: 0.7091 - tn: 25241.8027 - tp: 3076.0630
    Epoch 34: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7556 - categorical_crossentropy: 0.6664 - f1: 0.6762 - fn: 1264.5682 - fp: 777.0258 - loss: 0.4501 - precision: 0.7993 - recall: 0.7091 - tn: 25334.2656 - tp: 3087.3137 - val_categorical_accuracy: 0.6792 - val_categorical_crossentropy: 0.8876 - val_f1: 0.5827 - val_fn: 173.0000 - val_fp: 121.0000 - val_loss: 0.8876 - val_precision: 0.7173 - val_recall: 0.6396 - val_tn: 2759.0000 - val_tp: 307.0000 - learning_rate: 6.2500e-07
    Epoch 35/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7512 - categorical_crossentropy: 0.6707 - f1: 0.6645 - fn: 1261.0148 - fp: 789.3210 - loss: 0.4674 - precision: 0.7973 - recall: 0.7100 - tn: 25321.9707 - tp: 3090.8672 - val_categorical_accuracy: 0.6792 - val_categorical_crossentropy: 0.8794 - val_f1: 0.5818 - val_fn: 172.0000 - val_fp: 120.0000 - val_loss: 0.8794 - val_precision: 0.7196 - val_recall: 0.6417 - val_tn: 2760.0000 - val_tp: 308.0000 - learning_rate: 3.1250e-07
    Epoch 36/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7503 - categorical_crossentropy: 0.6738 - f1: 0.6628 - fn: 1272.6937 - fp: 805.8487 - loss: 0.4792 - precision: 0.7930 - recall: 0.7073 - tn: 25305.4434 - tp: 3079.1882 - val_categorical_accuracy: 0.6833 - val_categorical_crossentropy: 0.8794 - val_f1: 0.5843 - val_fn: 171.0000 - val_fp: 119.0000 - val_loss: 0.8794 - val_precision: 0.7220 - val_recall: 0.6438 - val_tn: 2761.0000 - val_tp: 309.0000 - learning_rate: 3.1250e-07
    Epoch 37/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - categorical_accuracy: 0.7532 - categorical_crossentropy: 0.6635 - f1: 0.6666 - fn: 1252.4037 - fp: 772.3370 - loss: 0.4539 - precision: 0.7998 - recall: 0.7101 - tn: 25243.6621 - tp: 3083.5962
    Epoch 37: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 227ms/step - categorical_accuracy: 0.7532 - categorical_crossentropy: 0.6636 - f1: 0.6666 - fn: 1257.0516 - fp: 775.2141 - loss: 0.4539 - precision: 0.7998 - recall: 0.7101 - tn: 25336.0781 - tp: 3094.8303 - val_categorical_accuracy: 0.6812 - val_categorical_crossentropy: 0.8771 - val_f1: 0.5834 - val_fn: 172.0000 - val_fp: 120.0000 - val_loss: 0.8771 - val_precision: 0.7196 - val_recall: 0.6417 - val_tn: 2760.0000 - val_tp: 308.0000 - learning_rate: 3.1250e-07
    Epoch 38/50
    270/270 ━━━━━━━━━━━━━━━━━━━━ 61s 226ms/step - categorical_accuracy: 0.7621 - categorical_crossentropy: 0.6609 - f1: 0.6875 - fn: 1233.0627 - fp: 775.3579 - loss: 0.4441 - precision: 0.8025 - recall: 0.7173 - tn: 25335.9336 - tp: 3118.8191 - val_categorical_accuracy: 0.6812 - val_categorical_crossentropy: 0.8824 - val_f1: 0.5872 - val_fn: 173.0000 - val_fp: 119.0000 - val_loss: 0.8824 - val_precision: 0.7207 - val_recall: 0.6396 - val_tn: 2761.0000 - val_tp: 307.0000 - learning_rate: 1.5625e-07
:::
:::

::: {.cell .code execution_count="104" execution="{\"iopub.execute_input\":\"2024-03-16T00:42:07.166177Z\",\"iopub.status.busy\":\"2024-03-16T00:42:07.165382Z\",\"iopub.status.idle\":\"2024-03-16T00:42:07.582843Z\",\"shell.execute_reply\":\"2024-03-16T00:42:07.581964Z\",\"shell.execute_reply.started\":\"2024-03-16T00:42:07.166142Z\"}" trusted="true"}
``` python
plot_history(history)
```

::: {.output .display_data}
![](vertopal_80011186bb6f445e819ecd831a034c72/d043da6dbcd6438e70d5bea0847d61461b15fc77.png)
:::
:::

::: {.cell .code execution_count="117" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:49.455395Z\",\"iopub.status.busy\":\"2024-03-16T00:43:49.454624Z\",\"iopub.status.idle\":\"2024-03-16T00:43:50.469740Z\",\"shell.execute_reply\":\"2024-03-16T00:43:50.468917Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:49.455363Z\"}" trusted="true"}
``` python
# This is a (1732, 8) NumPy array of raw probabilities
test_predictions_baseline = model.predict(test_ds, batch_size=BATCH_SIZE)

# Convert raw probabilities into actual class predictions
predicted_classes = np.argmax(test_predictions_baseline, axis=1)  # (1732,)
```

::: {.output .stream .stdout}
    15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step
:::
:::

::: {.cell .code execution_count="118" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:52.043814Z\",\"iopub.status.busy\":\"2024-03-16T00:43:52.042950Z\",\"iopub.status.idle\":\"2024-03-16T00:43:52.048813Z\",\"shell.execute_reply\":\"2024-03-16T00:43:52.047829Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:52.043779Z\"}" trusted="true"}
``` python
print(predicted_classes.shape)
print(predicted_classes[:10])
```

::: {.output .stream .stdout}
    (480,)
    [4 6 5 5 5 5 1 1 5 5]
:::
:::

::: {.cell .code execution_count="119" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:53.857175Z\",\"iopub.status.busy\":\"2024-03-16T00:43:53.856534Z\",\"iopub.status.idle\":\"2024-03-16T00:43:53.869696Z\",\"shell.execute_reply\":\"2024-03-16T00:43:53.868771Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:53.857145Z\"}" trusted="true"}
``` python
# Need to get the true labels from the test_ds dataset

# Initialize an empty list to store the labels
true_labels = []

# Iterate over the dataset batches
for _, tmp_labels in test_ds:
    # The "_" ignores the features, and "labels" are what we want to collect
    
    # Convert the labels to numpy arrays and append to the list
    true_labels.append(tmp_labels.numpy())

# True labels is a list of batches, need to 
# concatenate all the labels into a single numpy array
true_labels = np.concatenate(true_labels)

# Convert 1-hot labels to class labels
true_labels = np.argmax(true_labels, axis=1)  # (1732,)
```
:::

::: {.cell .code execution_count="120" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:56.327703Z\",\"iopub.status.busy\":\"2024-03-16T00:43:56.327003Z\",\"iopub.status.idle\":\"2024-03-16T00:43:56.332461Z\",\"shell.execute_reply\":\"2024-03-16T00:43:56.331498Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:56.327668Z\"}" trusted="true"}
``` python
print(true_labels.shape)
print(true_labels[:10])
```

::: {.output .stream .stdout}
    (480,)
    [5 6 5 5 5 5 1 1 5 5]
:::
:::

::: {.cell .code execution_count="121" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:57.489688Z\",\"iopub.status.busy\":\"2024-03-16T00:43:57.489278Z\",\"iopub.status.idle\":\"2024-03-16T00:43:57.496179Z\",\"shell.execute_reply\":\"2024-03-16T00:43:57.494946Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:57.489657Z\"}" trusted="true"}
``` python
import numpy as np

# Assuming predicted_classes and true_labels are numpy arrays
predicted_classes = np.array(predicted_classes)
true_labels = np.array(true_labels)

# Calculate accuracy
accuracy = np.mean(predicted_classes == true_labels)
print(accuracy)
```

::: {.output .stream .stdout}
    0.7145833333333333
:::
:::

::: {.cell .code execution_count="122" execution="{\"iopub.execute_input\":\"2024-03-16T00:43:59.522337Z\",\"iopub.status.busy\":\"2024-03-16T00:43:59.521943Z\",\"iopub.status.idle\":\"2024-03-16T00:43:59.528088Z\",\"shell.execute_reply\":\"2024-03-16T00:43:59.527207Z\",\"shell.execute_reply.started\":\"2024-03-16T00:43:59.522307Z\"}" trusted="true"}
``` python

count = 0

# Iterate over elements and update count
for true_label, predicted_class in zip(true_labels, predicted_classes):
    if true_label == predicted_class:
        count += 1

print("Total number of common values:", count)
```

::: {.output .stream .stdout}
    Total number of common values: 343
:::
:::

::: {.cell .code}
``` python
```
:::
